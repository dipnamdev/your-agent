web_scraper_ai_agent/
â”‚
â”œâ”€â”€ uv.py                     # Main entry point to run the tool
â”œâ”€â”€ config.py                 # Configuration: concurrency, delays, storage options, max depth
â”œâ”€â”€ requirements.txt          # Python dependencies (playwright, aiohttp, asyncio, sqlite3, etc.)
â”‚
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler.py             # Core crawler: discovers links, adds to queue
â”‚   â”œâ”€â”€ queue_manager.py       # Central queue + visited URL tracking
â”‚   â””â”€â”€ url_utils.py           # URL normalization, domain checks
â”‚
â”œâ”€â”€ scraper/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scraper_worker.py      # Async scraper worker: takes URL from queue, scrapes content
â”‚   â”œâ”€â”€ content_extractor.py   # Extracts main content, metadata, images, page type
â”‚   â””â”€â”€ playwright_utils.py    # Playwright setup, browser/page context helpers
â”‚
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db_manager.py          # SQLite DB handling, table creation, inserts
â”‚   â””â”€â”€ models.py              # Data model: page_url, title, content, meta, links, page_type
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py              # Logging setup for scraping progress/errors
â”‚   â”œâ”€â”€ retry.py               # Retry logic for failed requests
â”‚   â””â”€â”€ helpers.py             # Misc helper functions (delays, randomization)
â”‚
â”œâ”€â”€ data/                      # Optional: store temporary JSON dumps if needed
â”‚   â””â”€â”€ ...
â”‚
â””â”€â”€ README.md 


```markdown
. ðŸ“‚ Project_YA
â””â”€â”€ ðŸ“‚ __pycache__/
â”œâ”€â”€ ðŸ“„ config.py
â””â”€â”€ ðŸ“‚ data/
â”‚  â”œâ”€â”€ ðŸ“„ scraper.db
â””â”€â”€ ðŸ“‚ data_processing/
â”‚  â”œâ”€â”€ ðŸ“„ __init__.py
â”‚  â”œâ”€â”€ ðŸ“„ chunk.py
â”‚  â”œâ”€â”€ ðŸ“„ clean.py
â”‚  â”œâ”€â”€ ðŸ“„ embed.py
â”‚  â”œâ”€â”€ ðŸ“„ normalize.py
â”‚  â”œâ”€â”€ ðŸ“„ pipeline.py
â”‚  â”œâ”€â”€ ðŸ“„ save.py
â”œâ”€â”€ ðŸ“„ folder-structure.txt
â”œâ”€â”€ ðŸ“„ main.py
â”œâ”€â”€ ðŸ“„ pyproject.toml
â”œâ”€â”€ ðŸ“„ readme.md
â”œâ”€â”€ ðŸ“„ requirements.txt
â””â”€â”€ ðŸ“‚ utils/
â”‚  â”œâ”€â”€ ðŸ“„ __init__.py
â”‚  â””â”€â”€ ðŸ“‚ __pycache__/
â”‚  â”œâ”€â”€ ðŸ“„ helpers.py
â”‚  â”œâ”€â”€ ðŸ“„ logger.py
â”‚  â”œâ”€â”€ ðŸ“„ retry.py
â”œâ”€â”€ ðŸ“„ uv.lock
â””â”€â”€ ðŸ“‚ web_scraper/
â”‚  â””â”€â”€ ðŸ“‚ __pycache__/
â”‚  â””â”€â”€ ðŸ“‚ crawler/
â”‚    â”œâ”€â”€ ðŸ“„ __init__.py
â”‚    â””â”€â”€ ðŸ“‚ __pycache__/
â”‚    â”œâ”€â”€ ðŸ“„ crawler.py
â”‚    â”œâ”€â”€ ðŸ“„ queue_manager.py
â”‚    â”œâ”€â”€ ðŸ“„ url_utils.py
â”‚  â”œâ”€â”€ ðŸ“„ folder-structure.txt
â”‚  â””â”€â”€ ðŸ“‚ output/
â”‚    â”œâ”€â”€ ðŸ“„ scraped_worker_0.txt
â”‚    â”œâ”€â”€ ðŸ“„ scraped_worker_1.txt
â”‚    â”œâ”€â”€ ðŸ“„ scraped_worker_2.txt
â”‚    â”œâ”€â”€ ðŸ“„ scraped_worker_3.txt
â”‚    â”œâ”€â”€ ðŸ“„ scraped_worker_4.txt
â”‚  â”œâ”€â”€ ðŸ“„ readme.md
â”‚  â””â”€â”€ ðŸ“‚ scraper/
â”‚    â”œâ”€â”€ ðŸ“„ __init__.py
â”‚    â””â”€â”€ ðŸ“‚ __pycache__/
â”‚    â”œâ”€â”€ ðŸ“„ content_extractor.py
â”‚    â”œâ”€â”€ ðŸ“„ playwright_utils.py
â”‚    â”œâ”€â”€ ðŸ“„ scraper_worker.py
â”‚  â””â”€â”€ ðŸ“‚ storage/
â”‚    â”œâ”€â”€ ðŸ“„ __init__.py
â”‚    â””â”€â”€ ðŸ“‚ __pycache__/
â”‚    â”œâ”€â”€ ðŸ“„ db_manager.py
â”‚    â”œâ”€â”€ ðŸ“„ models.py
â”‚  â”œâ”€â”€ ðŸ“„ uv.py
â”‚  â””â”€â”€ ðŸ“„ view_db.py
```